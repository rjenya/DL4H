{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7b04a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"./resource\"\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "842d1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_pkl(path, obj):\n",
    "  with open(path, 'wb') as f:\n",
    "    pickle.dump(obj, f)\n",
    "    print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_pkl(path):\n",
    "  with open(path,'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    print(\" [*] load %s\" % path)\n",
    "    return obj\n",
    "\n",
    "def save_npy(path, obj):\n",
    "  np.save(path, obj)\n",
    "  print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_npy(path):\n",
    "  obj = np.load(path)\n",
    "  print(\" [*] load %s\" % path)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "85731ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load ./resource/vocab.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = load_pkl(DATA_PATH + '/vocab.pkl')\n",
    "TOTAL_NUM_CODES = len(vocab)\n",
    "TOTAL_NUM_CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "30d76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._data = load_pkl(DATA_PATH + '/data.pkl')\n",
    "        self._label = load_pkl(DATA_PATH + '/label.pkl')\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" return the number of samples (i.e. patients). \"\"\"\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self._data[index]\n",
    "        label = self._label[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c7f1b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load ./resource/data.pkl\n",
      " [*] load ./resource/label.pkl\n",
      "Size of dataset: 3000\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "print('Size of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5e26f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "\n",
    "    y = torch.zeros((num_patients, max_num_visits), dtype=torch.float)\n",
    "\n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    l = torch.zeros((num_patients), dtype=torch.long)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            x[i_patient,j_visit,0:len(visit)] = torch.Tensor(visit)\n",
    "            masks[i_patient,j_visit,0:len(visit)] = torch.ones(len(visit))\n",
    "            rev_j = len(patient) - j_visit - 1\n",
    "            rev_x[i_patient,rev_j,0:len(visit)] = torch.Tensor(visit)\n",
    "            rev_masks[i_patient,rev_j,0:len(visit)] = torch.ones(len(visit))\n",
    "            y[i_patient,j_visit] = labels[i_patient][j_visit]\n",
    "        l[i_patient] = len(patient)\n",
    "\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "851c2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 2400\n",
      "Length of val dataset: 600\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4156c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "     \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fe270b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    x_masked = x * torch.unsqueeze(masks,3)\n",
    "    return torch.sum(x_masked, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "aac90caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(hidden_states, masks):\n",
    "    a = torch.sum(masks, axis = 2)\n",
    "    a = a>0\n",
    "    a = a.unsqueeze(-1)\n",
    "\n",
    "    return torch.mul(hidden_states,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "94fee7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbRNN(\n",
       "  (embedding): Embedding(490, 256)\n",
       "  (rnn): GRU(256, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE=128\n",
    "EMBEDDING_DIM=256\n",
    "\n",
    "\n",
    "class EmbRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        self.embedding = None\n",
    "        self.rnn = None\n",
    "        self.rev_rnn = None\n",
    "        self.fc = None\n",
    "        self.sigmoid = None\n",
    "        \n",
    "        # your code here\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=EMBEDDING_DIM)\n",
    "        self.rnn = nn.GRU(input_size=EMBEDDING_DIM,hidden_size=HIDDEN_SIZE, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=HIDDEN_SIZE,hidden_size=HIDDEN_SIZE, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=2*HIDDEN_SIZE,out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n= applyMask(output,masks)\n",
    "        \n",
    "        true_h_n_rev = None\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        rev_output, _ = self.rnn(rev_x)\n",
    "        true_h_n_rev = applyMask(rev_output,rev_masks)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 2))  \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.squeeze(-1)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "model = EmbRNN(num_codes = TOTAL_NUM_CODES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3b0d7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(),lr=LEARNING_RATE)\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#input: Y_score,Y_pred,Y_true\n",
    "#output: accuracy, auc, precision, recall, f1-score\n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "#input: model, loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.LongTensor()\n",
    "    all_y_pred = torch.LongTensor()\n",
    "    all_y_score = torch.FloatTensor()\n",
    "    for x, masks, rev_x, rev_masks, y, l in loader:\n",
    "        # pass the input through the model\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_pred = (y_hat > 0.5).type(torch.float)\n",
    "        for i in range(y.shape[0]):\n",
    "            all_y_true = torch.cat((all_y_true, y[i,:l[i]].to('cpu').flatten()), dim=0)\n",
    "            all_y_pred = torch.cat((all_y_pred,  y_pred[i,:l[i]].to('cpu').flatten()), dim=0)\n",
    "            all_y_score = torch.cat((all_y_score,  y_hat[i,:l[i]].to('cpu').flatten()), dim=0)\n",
    "        \n",
    "    acc, auc, precision, recall, f1 = classification_metrics(all_y_score.detach().numpy(), \n",
    "                                                             all_y_pred.detach().numpy(), \n",
    "                                                             all_y_true.detach().numpy())\n",
    "    print(f\"acc: {acc:.3f}, auc: {auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1dbf1076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.665273\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "Epoch: 2 \tTraining Loss: 0.665340\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "Epoch: 3 \tTraining Loss: 0.665327\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "Epoch: 4 \tTraining Loss: 0.665236\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "Epoch: 5 \tTraining Loss: 0.665427\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "Epoch: 6 \tTraining Loss: 0.665294\n",
      "acc: 0.628, auc: 0.498, precision: 0.235, recall: 0.281, f1: 0.256\n",
      "acc: 0.637, auc: 0.505, precision: 0.236, recall: 0.276, f1: 0.254\n",
      "943.9 secs 5110.2 MByte\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import resource \n",
    "\n",
    "n_epochs = 6\n",
    "time_start = time.perf_counter()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x, masks, rev_x, rev_masks, y, l in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    evaluate(model, train_loader)\n",
    "    evaluate(model, val_loader)\n",
    "    \n",
    "time_elapsed = (time.perf_counter() - time_start)\n",
    "memMb=resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0\n",
    "print (\"%5.1f secs %5.1f MByte\" % (time_elapsed,memMb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de76c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
