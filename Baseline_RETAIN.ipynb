{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bbf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "DATA_PATH = \"./resource\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0234abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_pkl(path, obj):\n",
    "  with open(path, 'wb') as f:\n",
    "    pickle.dump(obj, f)\n",
    "    print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_pkl(path):\n",
    "  with open(path,'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    print(\" [*] load %s\" % path)\n",
    "    return obj\n",
    "\n",
    "def save_npy(path, obj):\n",
    "  np.save(path, obj)\n",
    "  print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_npy(path):\n",
    "  obj = np.load(path)\n",
    "  print(\" [*] load %s\" % path)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dcc980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load ./resource/vocab.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = load_pkl(DATA_PATH + '/vocab.pkl')\n",
    "TOTAL_NUM_CODES = len(vocab)\n",
    "TOTAL_NUM_CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c65867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._data = load_pkl(DATA_PATH + '/data.pkl')\n",
    "        self._label = load_pkl(DATA_PATH + '/label.pkl')\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" return the number of samples (i.e. patients). \"\"\"\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self._data[index]\n",
    "        label = self._label[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df92c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load ./resource/data.pkl\n",
      " [*] load ./resource/label.pkl\n",
      "Size of dataset: 3000\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "print('Size of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9a2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "\n",
    "    y = torch.zeros((num_patients, max_num_visits), dtype=torch.float)\n",
    "\n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    l = torch.zeros((num_patients), dtype=torch.long)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            x[i_patient,j_visit,0:len(visit)] = torch.Tensor(visit)\n",
    "            masks[i_patient,j_visit,0:len(visit)] = torch.ones(len(visit))\n",
    "            rev_j = len(patient) - j_visit - 1\n",
    "            rev_x[i_patient,rev_j,0:len(visit)] = torch.Tensor(visit)\n",
    "            rev_masks[i_patient,rev_j,0:len(visit)] = torch.ones(len(visit))\n",
    "            y[i_patient,j_visit] = labels[i_patient][j_visit]\n",
    "        l[i_patient] = len(patient)\n",
    "\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679c23bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 2400\n",
      "Length of val dataset: 600\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2b9d2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33261c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        alpha = torch.softmax(self.a_att(g),dim=1)\n",
    "        return alpha\n",
    "    \n",
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        beta = torch.tanh(self.b_att(h))\n",
    "        return beta\n",
    "def apply_attention(alpha, beta, rev_v, rev_masks):\n",
    "    rev_masks = torch.sum(rev_masks,dim=2)\n",
    "    rev_masks = rev_masks > 0\n",
    "    rev_masks = rev_masks.unsqueeze(-1)\n",
    "    rev_v = rev_v * rev_masks\n",
    "    #c = torch.sum(beta * alpha * rev_v, dim =1)\n",
    "    c = beta * alpha * rev_v\n",
    "    return c\n",
    "def sum_embeddings_with_mask(x, masks):\n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06c5238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RETAIN(\n",
       "  (embedding): Embedding(490, 256)\n",
       "  (rnn_a): GRU(256, 256, batch_first=True)\n",
       "  (rnn_b): GRU(256, 256, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (b_att): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM=256\n",
    "\n",
    "\n",
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=EMBEDDING_DIM):\n",
    "        super().__init__()\n",
    "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
    "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_a = nn.GRU(input_size=embedding_dim,hidden_size=embedding_dim, batch_first=True)\n",
    "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        self.rnn_b = nn.GRU(input_size=embedding_dim,hidden_size=embedding_dim, batch_first=True)\n",
    "        # Define the alpha-attention using `AlphaAttention()`;\n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        # Define the beta-attention using `BetaAttention()`;\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        # Define the linear layers using `nn.Linear()`;\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        # Define the final activation layer using `nn.Sigmoid().\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        # 1. Pass the reversed sequence through the embedding layer;\n",
    "        #print(rev_x.shape)\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        #print(rev_x.shape)\n",
    "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        #print(g.shape)\n",
    "        #print(h.shape)\n",
    "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        #print(alpha.shape)\n",
    "        #print(beta.shape)\n",
    "        # 5. Sum the attention up using `attention_sum()`;\n",
    "        c = apply_attention(alpha, beta, rev_x, rev_masks)\n",
    "        #print(c.shape)\n",
    "        # 6. Pass the context vector through the linear and activation layers.\n",
    "        logits = self.fc(c)\n",
    "        #print(logits.shape)\n",
    "        probs = self.sigmoid(logits)\n",
    "        #probs = torch.softmax(logits,dim=1)\n",
    "        #print(probs.shape)\n",
    "        return probs.squeeze()\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "retain = RETAIN(num_codes = TOTAL_NUM_CODES)\n",
    "retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6ce7b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "#input: Y_score,Y_pred,Y_true\n",
    "#output: accuracy, auc, precision, recall, f1-score\n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y, l in val_loader:\n",
    "        y_logit = model(x, masks, rev_x, rev_masks)\n",
    "#        print(y_logit)\n",
    "        y_hat = (y_logit > 0.4).int()\n",
    "#        print(sum(y_hat))\n",
    "        for i in range(y.shape[0]):\n",
    "            y_score = torch.cat((y_score, y_logit[i,:l[i]].detach().to('cpu').flatten()), dim=0)\n",
    "            y_pred = torch.cat((y_pred,  y_hat[i,:l[i]].detach().to('cpu').flatten()), dim=0)\n",
    "            y_true = torch.cat((y_true,  y[i,:l[i]].detach().to('cpu').flatten()), dim=0)\n",
    "    \n",
    "    #p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    acc, roc_auc, p, r, f = classification_metrics(y_score.detach().numpy(), \n",
    "                                                             y_pred.detach().numpy(), \n",
    "                                                             y_true.detach().numpy())\n",
    "\n",
    "    #roc_auc = roc_auc_score(y_true, y_score)\n",
    "    return p, r, f, roc_auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "170e3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y,l in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        #evaluate(model, val_loader)\n",
    "        p, r, f, roc_auc, acc = eval(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}, acc: {:.2f}'.format(epoch+1, p, r, f, roc_auc, acc))\n",
    "\n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "21269ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.684651\n",
      "Epoch: 1 \t Validation p: 0.23, r:0.68, f: 0.34, roc_auc: 0.49, acc: 0.40\n",
      "Epoch: 2 \t Training Loss: 0.640114\n",
      "Epoch: 2 \t Validation p: 0.23, r:0.42, f: 0.30, roc_auc: 0.51, acc: 0.56\n",
      "Epoch: 3 \t Training Loss: 0.601998\n",
      "Epoch: 3 \t Validation p: 0.25, r:0.22, f: 0.23, roc_auc: 0.56, acc: 0.67\n",
      "Epoch: 4 \t Training Loss: 0.571388\n",
      "Epoch: 4 \t Validation p: 0.34, r:0.21, f: 0.26, roc_auc: 0.60, acc: 0.73\n",
      "Epoch: 5 \t Training Loss: 0.546833\n",
      "Epoch: 5 \t Validation p: 0.38, r:0.26, f: 0.31, roc_auc: 0.63, acc: 0.74\n",
      "Epoch: 6 \t Training Loss: 0.525029\n",
      "Epoch: 6 \t Validation p: 0.40, r:0.17, f: 0.24, roc_auc: 0.62, acc: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "retain = RETAIN(num_codes = TOTAL_NUM_CODES)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 6\n",
    "train(retain, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f5717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
