{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ec49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"./%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    projector_embed_dim = 100\n",
    "    generator_embed_dim = n_hidden\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 5\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode\n",
    "\n",
    "    log_dir = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0264a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def save_pkl(path, obj):\n",
    "  with open(path, 'wb') as f:\n",
    "    pickle.dump(obj, f)\n",
    "    print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_pkl(path):\n",
    "  with open(path,'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    print(\" [*] load %s\" % path)\n",
    "    return obj\n",
    "\n",
    "def save_npy(path, obj):\n",
    "  np.save(path, obj)\n",
    "  print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_npy(path):\n",
    "  obj = np.load(path)\n",
    "  print(\" [*] load %s\" % path)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876be5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "max_visit_size = 300\n",
    "\n",
    "\n",
    "class PatientReader(object):\n",
    "    def __init__(self, config):\n",
    "        self.data_path = data_path = config.data_path\n",
    "\n",
    "        self.vocab_path = vocab_path = os.path.join(data_path, \"vocab.pkl\")\n",
    "\n",
    "        # use train data to build vocabulary\n",
    "        if os.path.exists(vocab_path):\n",
    "            self._load()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_train_patients = math.ceil((len(self.X_train_data) + 0.0))\n",
    "        self.n_valid_patients = math.ceil((len(self.X_valid_data) + 0.0))\n",
    "        self.n_test_patients = math.ceil((len(self.X_test_data) + 0.0))\n",
    "\n",
    "        self.lda_vocab_size = config.lda_vocab_size\n",
    "        self.n_stops = config.n_stops\n",
    "\n",
    "        self.idx2word = {v: k for k, v in self.vocab.items()} #needed to go from index to concept \n",
    "\n",
    "        print(\"vocabulary size: {}\".format(self.vocab_size))\n",
    "        print(\"number of training documents: {}\".format(self.n_train_patients))\n",
    "        print(\"number of validation documents: {}\".format(self.n_valid_patients))\n",
    "        print(\"number of testing documents: {}\".format(self.n_test_patients))\n",
    "\n",
    "    def _load(self):\n",
    "        self.vocab = load_pkl(self.vocab_path)\n",
    "\n",
    "        self.X_train_data = load_pkl(self.data_path + '/' + 'X_train' + '.pkl')\n",
    "        self.Y_train_data = load_pkl(self.data_path + '/' + 'Y_train' + '.pkl')\n",
    "\n",
    "        self.X_valid_data = load_pkl(self.data_path + '/' + 'X_valid' + '.pkl')\n",
    "        self.Y_valid_data = load_pkl(self.data_path + '/' + 'Y_valid' + '.pkl')\n",
    "\n",
    "        self.X_test_data = load_pkl(self.data_path + '/' + 'X_test' + '.pkl')\n",
    "        self.Y_test_data = load_pkl(self.data_path + '/' + 'Y_test' + '.pkl')\n",
    "\n",
    "    def get_data_from_type(self, data_type):\n",
    "        if data_type == \"train\":\n",
    "            X_raw_data = self.X_train_data\n",
    "            Y_raw_data = self.Y_train_data\n",
    "        elif data_type == \"valid\":\n",
    "            X_raw_data = self.X_valid_data\n",
    "            Y_raw_data = self.Y_valid_data\n",
    "        elif data_type == \"test\":\n",
    "            X_raw_data = self.X_test_data\n",
    "            Y_raw_data = self.Y_test_data\n",
    "        else:\n",
    "            raise Exception(\" [!] Unkown data type %s: %s\" % data_type)\n",
    "\n",
    "        return X_raw_data, Y_raw_data\n",
    "\n",
    "    def get_Xc(self, data):\n",
    "        \"\"\"data is a patient...a sequence of visits\n",
    "            so a list of lists...the outer list is of size T_patient\n",
    "            the inner lists contain the concepts within each visit\n",
    "        \"\"\"\n",
    "        patient = [concept for visit in data for concept in visit]\n",
    "        patient = [x-1 for x in patient] \n",
    "        counts = np.bincount(patient, minlength=self.vocab_size)\n",
    "        stops_flag = np.array(list(np.ones([self.lda_vocab_size], dtype=np.int32)) +\n",
    "                              list(np.zeros([self.n_stops], dtype=np.int32)))\n",
    "\n",
    "        return counts * stops_flag\n",
    "\n",
    "    def get_X(self, data):\n",
    "        \"\"\"\n",
    "        data is a list of lists of different length\n",
    "        return an array of shape CxT where \n",
    "        entry Mij = ci if ci in visit j\n",
    "        \"\"\"\n",
    "        T_patient = len(data)\n",
    "        res = np.zeros([self.vocab_size, T_patient])\n",
    "        for i in range(self.vocab_size):\n",
    "            for j in range(T_patient):\n",
    "                if (i+1) in data[j]:\n",
    "                    res[i, j] = (i+1)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def iterator(self, data_type=\"train\"):\n",
    "        \"\"\"\n",
    "        goes over the data and\n",
    "        returns X, Xc, Y, and seq_len in a round robin\n",
    "        seq_len is a vector of size C where each \n",
    "        entry is T_patient\n",
    "        \"\"\"\n",
    "        X_raw_data, Y_raw_data = self.get_data_from_type(data_type)\n",
    "\n",
    "        x_infos = itertools.cycle(([self.get_X(X_doc[:max_visit_size]), self.get_Xc(X_doc[:max_visit_size])]\n",
    "                                   for X_doc in X_raw_data if X_doc != []))\n",
    "        y_infos = itertools.cycle(([Y_doc[:max_visit_size], np.array([len(Y_doc[:max_visit_size])]*self.vocab_size)]\n",
    "                                   for Y_doc in Y_raw_data if Y_doc != []))\n",
    "\n",
    "        return x_infos, y_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9a2ec568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "class GaussianSampleLayer(lasagne.layers.MergeLayer):\n",
    "    def __init__(self, mu, logsigma, maxlen, **kwargs):\n",
    "        self.rng = RandomStreams(lasagne.random.get_rng().randint(1,2147462579))\n",
    "        self.maxlen = maxlen\n",
    "        super(GaussianSampleLayer, self).__init__([mu, logsigma], **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "               self.maxlen,\n",
    "               self.input_shapes[0][1] or inputs[0].shape[1]) \n",
    "        #print(shape)\n",
    "        return shape\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "               self.maxlen,\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        return mu + T.exp(logsigma) * self.rng.normal(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cea0875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#from lasagne.layers.timefusion import MaskingLayer\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, precision_recall_curve\n",
    "#from lasagne.layers.theta import ThetaLayer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import average_precision_score as pr_auc\n",
    "\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "\n",
    "# Number of training sequences in each batch\n",
    "\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "def iterate_minibatches_listinputs(inputs, batchsize, shuffle=False):\n",
    "    assert inputs is not None\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [input[excerpt] for input in inputs]\n",
    "\n",
    "\n",
    "\n",
    "def CONTENT(data_sets,N_HIDDEN = 200,embedsize = 100,n_topics = 50,num_epochs = 6,N_BATCH = 1):\n",
    "    # Optimization learning rate\n",
    "    LEARNING_RATE = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "    eta_decay = np.array(0.5, dtype=theano.config.floatX)\n",
    "    # Min/max sequence length\n",
    "    MAX_LENGTH = 300\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(\"train\")\n",
    "    trainingAdmiSeqs, trainingMask, trainingLabels, trainingLengths, ltr = prepare_data(X_raw_data, Y_raw_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "    Num_Samples, MAX_LENGTH, N_VOCAB = trainingAdmiSeqs.shape\n",
    "\n",
    "    X_valid_data, Y_valid_data = data_sets.get_data_from_type(\"valid\")\n",
    "    validAdmiSeqs, validMask, validLabels, validLengths, lval  = prepare_data(X_valid_data, Y_valid_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "\n",
    "    X_test_data, Y_test_data = data_sets.get_data_from_type(\"test\")\n",
    "    test_admiSeqs, test_mask, test_labels, testLengths, ltes = prepare_data(X_test_data, Y_test_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "    alllength = sum(trainingLengths) + sum(validLengths) + sum(testLengths)\n",
    "    print(alllength)\n",
    "    eventNum = sum(ltr)+sum(lval)+sum(ltes)\n",
    "    print(eventNum)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "    \n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, max sequence length, number of features)\n",
    "    l_in = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH, N_VOCAB))\n",
    "    #l_label = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH, 1))\n",
    "\n",
    "    # The network also needs a way to provide a mask for each sequence.  We'll\n",
    "    # use a separate input layer for that.  Since the mask only determines\n",
    "    # which indices are part of the sequence for each batch entry, they are\n",
    "    # supplied as matrices of dimensionality (N_BATCH, MAX_LENGTH)\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH))\n",
    "    \n",
    "    \n",
    "    #l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, W = W_embed, num_leading_axes=2)\n",
    "    l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, num_leading_axes=2)\n",
    "    #l_embed.params[l_embed.W].remove(\"trainable\")\n",
    "    #l_drop = lasagne.layers.dropout(l_embed)\n",
    "    l_forward = lasagne.layers.GRULayer(\n",
    "        l_embed, N_HIDDEN, mask_input=l_mask, grad_clipping=GRAD_CLIP,\n",
    "        only_return_final=False)\n",
    "\n",
    "    #l_forward = MaskingLayer([l_forward0, l_mask])\n",
    " \n",
    "    l_1 = lasagne.layers.DenseLayer(l_in, num_units=N_HIDDEN, nonlinearity=lasagne.nonlinearities.rectify, num_leading_axes=2)\n",
    "    l_2 = lasagne.layers.DenseLayer(l_1, num_units=N_HIDDEN, nonlinearity=lasagne.nonlinearities.rectify, num_leading_axes=2)\n",
    "    mu = lasagne.layers.DenseLayer(l_2, num_units=n_topics, nonlinearity=None, num_leading_axes=1)# batchsize * n_topic\n",
    "    log_sigma = lasagne.layers.DenseLayer(l_2, num_units=n_topics, nonlinearity=None, num_leading_axes=1)# batchsize * n_topic\n",
    "    #l_theta = ThetaLayer([mu,log_sigma],maxlen=MAX_LENGTH)#batchsize * maxlen * n_topic\n",
    "    l_theta = GaussianSampleLayer(mu,log_sigma,maxlen=MAX_LENGTH)\n",
    "\n",
    "    l_B = lasagne.layers.DenseLayer(l_in, b=None, num_units=n_topics, nonlinearity=None, num_leading_axes=2)\n",
    "    l_context = lasagne.layers.ElemwiseMergeLayer([l_B, l_theta],T.mul)\n",
    "    l_context = lasagne.layers.ExpressionLayer(l_context, lambda X: X.mean(-1), output_shape=\"auto\")\n",
    "\n",
    "    l_dense0 = lasagne.layers.DenseLayer(\n",
    "        l_forward, num_units=1, nonlinearity=None,num_leading_axes=2)\n",
    "    l_dense1 = lasagne.layers.reshape(l_dense0, ([0], [1]))#batchsize * maxlen\n",
    "    l_dense = lasagne.layers.ElemwiseMergeLayer([l_dense1, l_context],T.add)\n",
    "    l_out0 = lasagne.layers.NonlinearityLayer(l_dense, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    l_out = lasagne.layers.ExpressionLayer(lasagne.layers.ElemwiseMergeLayer([l_out0, l_mask],T.mul), lambda X:X+0.000001)\n",
    "\n",
    "\n",
    "\n",
    "    target_values = T.matrix('target_output')\n",
    "    target_values_flat = T.flatten(target_values)\n",
    "\n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "    # The network output will have shape (n_batch, maxlen); let's flatten to get a\n",
    "    # 1-dimensional vector of predicted values\n",
    "    predicted_values = network_output.flatten()\n",
    "    # Our cost will be mean-squared error\n",
    "    cost = lasagne.objectives.binary_crossentropy(predicted_values, target_values_flat)\n",
    "    #kl_term = l_theta.klterm\n",
    "    layer_outputs = lasagne.layers.get_output([mu, log_sigma])\n",
    "    z_mu =  layer_outputs[0]\n",
    "    z_ls =  layer_outputs[1]\n",
    "    kl_term = 0.5 * T.sum(1 + 2*z_ls - T.sqr(z_mu) - T.exp(2 * z_ls))\n",
    "    cost = cost.sum()-kl_term\n",
    "    \n",
    "    test_output = lasagne.layers.get_output(l_out, deterministic=True)\n",
    "\n",
    "    #cost = T.mean((predicted_values - target_values)**2)\n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(l_out)\n",
    "\n",
    "    # Compute SGD updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adam(cost, all_params, LEARNING_RATE)\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values, l_mask.input_var],\n",
    "                            cost, updates=updates)\n",
    "    compute_cost = theano.function(\n",
    "        [l_in.input_var, target_values, l_mask.input_var],cost)\n",
    "    prd = theano.function([l_in.input_var, l_mask.input_var], test_output)\n",
    "    #rnn_out = T.concatenate(l_theta.theta, lasagne.layers.get_output(l_forward0)[:,-1,:].reshape((N_BATCH, N_HIDDEN)),axis=1)\n",
    "    #output_theta = theano.function([l_in.input_var, l_mask.input_var], [l_theta.theta, lasagne.layers.get_output(l_forward0)[:,-1,:].reshape((N_BATCH, N_HIDDEN))], on_unused_input='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Training ...\")\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            start_time = time.time()\n",
    "            thetas_train = []\n",
    "            for batch in iterate_minibatches_listinputs([trainingAdmiSeqs, trainingLabels, trainingMask], N_BATCH,\n",
    "                                                        shuffle=True):\n",
    "                inputs = batch\n",
    "                train_err += train(inputs[0], inputs[1], inputs[2])\n",
    "                train_batches += 1\n",
    "                #theta_train, rnnvec_train = output_theta(inputs[0], inputs[2])\n",
    "                #rnnout_train = np.concatenate([theta_train, rnnvec_train], axis=1)\n",
    "                #thetas_train.append(rnnout_train.flatten())\n",
    "                #thetas_train.append(rnnvec_train.flatten())\n",
    "                #if (train_batches+1)% 1000 == 0:\n",
    "                #    print(train_batches)\n",
    "\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    " \n",
    "            # After training, we compute and print the test error:\n",
    "            test_err = 0\n",
    "\n",
    "            test_batches = 0\n",
    "            new_testlabels = []\n",
    "            pred_testlabels = []\n",
    "            thetas = []\n",
    "            for batch in iterate_minibatches_listinputs([test_admiSeqs, test_labels, test_mask, testLengths], 1, shuffle=False):\n",
    "                inputs = batch\n",
    "                err = compute_cost(inputs[0], inputs[1], inputs[2])\n",
    "                test_err += err\n",
    "                leng = inputs[3][0]\n",
    "                new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "                pred_testlabels.extend(prd(inputs[0], inputs[2]).flatten()[:leng])\n",
    "                #theta, rnnvec = output_theta(inputs[0], inputs[2])\n",
    "                #rnnout = np.concatenate([theta, rnnvec],axis=1)\n",
    "                #thetas.append(rnnout.flatten())\n",
    "                #thetas.append(rnnvec.flatten())\n",
    "                test_batches += 1\n",
    "            test_auc = roc_auc_score(new_testlabels, pred_testlabels)\n",
    "            test_pr_auc = pr_auc(new_testlabels, pred_testlabels)\n",
    " \n",
    "\n",
    "            test_pre_rec_f1 = precision_recall_fscore_support(np.array(new_testlabels), np.array(pred_testlabels)>0.5, average='binary')\n",
    "            test_acc = accuracy_score(np.array(new_testlabels), np.array(pred_testlabels)>0.5)\n",
    "            print(\"Final results:\")\n",
    "            print(\"  test loss:\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "            print(\"  test auc:\\t\\t{:.6f}\".format(test_auc))\n",
    "            print(\"  test pr_auc:\\t\\t{:.6f}\".format(test_pr_auc))\n",
    "            print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "                test_acc * 100))\n",
    "            print(\"  test Precision, Recall and F1:\\t\\t{:.4f} \\t\\t{:.4f}\\t\\t{:.4f}\".format(test_pre_rec_f1[0], test_pre_rec_f1[1], test_pre_rec_f1[2]))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "\n",
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(theano.config.floatX)\n",
    "    y = np.ones((n_samples, maxlen)).astype(theano.config.floatX)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj-1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx,:lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4215e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] load ./resource/vocab.pkl\n",
      " [*] load ./resource/X_train.pkl\n",
      " [*] load ./resource/Y_train.pkl\n",
      " [*] load ./resource/X_valid.pkl\n",
      " [*] load ./resource/Y_valid.pkl\n",
      " [*] load ./resource/X_test.pkl\n",
      " [*] load ./resource/Y_test.pkl\n",
      "vocabulary size: 619\n",
      "number of training documents: 2000\n",
      "number of validation documents: 500\n",
      "number of testing documents: 500\n",
      "239887\n",
      "665245\n",
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Epoch 1 of 6 took 557.066s\n",
      "  training loss:\t\t3081.478531\n",
      "Final results:\n",
      "  test loss:\t\t3070.321072\n",
      "  test auc:\t\t0.800792\n",
      "  test pr_auc:\t\t0.642164\n",
      "  test accuracy:\t\t83.48 %\n",
      "  test Precision, Recall and F1:\t\t0.7097 \t\t0.4435\t\t0.5458\n",
      "Epoch 2 of 6 took 558.447s\n",
      "  training loss:\t\t3079.344318\n",
      "Final results:\n",
      "  test loss:\t\t3069.769506\n",
      "  test auc:\t\t0.798518\n",
      "  test pr_auc:\t\t0.646326\n",
      "  test accuracy:\t\t83.80 %\n",
      "  test Precision, Recall and F1:\t\t0.7897 \t\t0.3764\t\t0.5099\n",
      "Epoch 3 of 6 took 570.463s\n",
      "  training loss:\t\t3078.690695\n",
      "Final results:\n",
      "  test loss:\t\t3069.722281\n",
      "  test auc:\t\t0.798902\n",
      "  test pr_auc:\t\t0.645477\n",
      "  test accuracy:\t\t83.76 %\n",
      "  test Precision, Recall and F1:\t\t0.7453 \t\t0.4166\t\t0.5344\n",
      "Epoch 4 of 6 took 676.977s\n",
      "  training loss:\t\t3077.955378\n",
      "Final results:\n",
      "  test loss:\t\t3069.910023\n",
      "  test auc:\t\t0.801071\n",
      "  test pr_auc:\t\t0.649544\n",
      "  test accuracy:\t\t83.52 %\n",
      "  test Precision, Recall and F1:\t\t0.7055 \t\t0.4526\t\t0.5515\n",
      "Epoch 5 of 6 took 721.600s\n",
      "  training loss:\t\t3077.118746\n",
      "Final results:\n",
      "  test loss:\t\t3069.969144\n",
      "  test auc:\t\t0.796134\n",
      "  test pr_auc:\t\t0.645841\n",
      "  test accuracy:\t\t83.75 %\n",
      "  test Precision, Recall and F1:\t\t0.7629 \t\t0.3975\t\t0.5227\n",
      "Epoch 6 of 6 took 698.834s\n",
      "  training loss:\t\t3076.175151\n",
      "Final results:\n",
      "  test loss:\t\t3070.317824\n",
      "  test auc:\t\t0.790936\n",
      "  test pr_auc:\t\t0.639503\n",
      "  test accuracy:\t\t83.37 %\n",
      "  test Precision, Recall and F1:\t\t0.7176 \t\t0.4234\t\t0.5326\n"
     ]
    }
   ],
   "source": [
    "C = Config()\n",
    "datasets = PatientReader(C)\n",
    "CONTENT(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0536abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239887\n",
      "665245\n",
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Epoch 1 of 1 took 438.894s\n",
      "  training loss:\t\t3081.710281\n",
      "Final results:\n",
      "  test loss:\t\t3070.177727\n",
      "  test auc:\t\t0.799458\n",
      "  test pr_auc:\t\t0.638642\n",
      "  test accuracy:\t\t83.56 %\n",
      "  test Precision, Recall and F1:\t\t0.7274 \t\t0.4244\t\t0.5361\n"
     ]
    }
   ],
   "source": [
    "#theano.config.exception_verbosity='high'\n",
    "CONTENT(datasets,N_HIDDEN = 150,embedsize = 50,n_topics = 50,num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "63c6f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239887\n",
      "665245\n",
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Epoch 1 of 1 took 1065.442s\n",
      "  training loss:\t\t3081.371253\n",
      "Final results:\n",
      "  test loss:\t\t3069.850572\n",
      "  test auc:\t\t0.800690\n",
      "  test pr_auc:\t\t0.641014\n",
      "  test accuracy:\t\t83.52 %\n",
      "  test Precision, Recall and F1:\t\t0.7361 \t\t0.4108\t\t0.5273\n"
     ]
    }
   ],
   "source": [
    "CONTENT(datasets,N_HIDDEN = 300,embedsize = 200,n_topics = 50,num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1bfa03af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239887\n",
      "665245\n",
      "Building network ...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Epoch 1 of 1 took 487.743s\n",
      "  training loss:\t\t3081.395941\n",
      "Final results:\n",
      "  test loss:\t\t3069.658085\n",
      "  test auc:\t\t0.800571\n",
      "  test pr_auc:\t\t0.644693\n",
      "  test accuracy:\t\t83.63 %\n",
      "  test Precision, Recall and F1:\t\t0.7817 \t\t0.3729\t\t0.5049\n"
     ]
    }
   ],
   "source": [
    "CONTENT(datasets,N_HIDDEN = 100,embedsize = 100,n_topics = 100,num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68da4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
